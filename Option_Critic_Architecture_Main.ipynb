{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "is_ipython = True\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import \n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.option_critic_network import OptionsNetwork\n",
    "from helper.buffer import ReplayBuffer\n",
    "from helper.state_processor import StateProcessor\n",
    "\n",
    "#execfile(\"models/option_critic_network.py\")\n",
    "#execfile(\"helper/buffer.py\")\n",
    "#execfile(\"helper/state_processor.py\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "#   Utility Parameters\n",
    "# ========================================\n",
    "# Render gym env during training\n",
    "RENDER_ENV = True\n",
    "# Use Gym Monitor\n",
    "GYM_MONITOR_EN = True\n",
    "# Gym environment\n",
    "ENV_NAME = 'Pong-v0'\n",
    "# Directory for storing gym results\n",
    "MONITOR_DIR = '../results2/gym_ddpg'\n",
    "# Directory for storing tensorboard summary results\n",
    "SUMMARY_DIR = '../results2/tf_ddpg'\n",
    "# Seed\n",
    "RANDOM_SEED = 1234\n",
    "# np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ==========================\n",
    "#   Training Parameters\n",
    "# ==========================\n",
    "# Update Frequency\n",
    "update_freq = 4\n",
    "# Max training steps\n",
    "MAX_EPISODES = 8000\n",
    "# Max episode length\n",
    "MAX_EP_STEPS = 250000\n",
    "# Maximum frames per game\n",
    "MAX_STEPS = 18000\n",
    "# Base learning rate for the Actor network\n",
    "ACTOR_LEARNING_RATE = 0.00025\n",
    "# Base learning rate for the Critic Network\n",
    "CRITIC_LEARNING_RATE = 0.00025\n",
    "# Contributes to the nitial random walk\n",
    "MAX_START_ACTION_ATTEMPTS = 30\n",
    "# Update params\n",
    "FREEZE_INTERVAL = 10000\n",
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "# Soft target update param\n",
    "TAU = 0.001\n",
    "# Starting chance of random action\n",
    "START_EPS = 1\n",
    "# Final chance of random action\n",
    "END_EPS = 0.1\n",
    "# How many steps of training to reduce startE to endE.\n",
    "ANNEALING = 1000000\n",
    "# Number of options\n",
    "OPTION_DIM = 8\n",
    "# Pretrain steps\n",
    "PRE_TRAIN_STEPS = 50000\n",
    "# Size of replay buffer\n",
    "BUFFER_SIZE = 1000000\n",
    "# Minibatch size\n",
    "MINIBATCH_SIZE = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "#   Tensorflow Summary Opself.model\n",
    "# ===========================\n",
    "\n",
    "\n",
    "def build_summaries():\n",
    "    summary_ops = tf.Summary()\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"DOCA/Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"DOCA/Qmax Value\", episode_ave_max_q)\n",
    "    episode_termination_ratio = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"DOCA/Term Ratio\", episode_termination_ratio)\n",
    "    tot_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"DOCA/Total Reward\", tot_reward)\n",
    "    cum_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"DOCA/Cummulative Reward\", tot_reward)\n",
    "    rmng_frames = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"DOCA/Remaining Frames\", rmng_frames)\n",
    "\n",
    "    summary_vars = [\n",
    "        episode_reward, episode_ave_max_q,\n",
    "        episode_termination_ratio, tot_reward, cum_reward, rmng_frames]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "\n",
    "def get_reward(reward):\n",
    "    if reward < 0:\n",
    "        score = -1\n",
    "    elif reward > 0:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    return score, reward\n",
    "\n",
    "def get_epsilon(frm_count):\n",
    "    #linear descent from 1 to 0.1 starting at the replay_start_time\n",
    "    replay_start_time = max([float(frm_count)-PRE_TRAIN_STEPS, 0])\n",
    "    epsilon = START_EPS\n",
    "    epsilon -= (START_EPS - END_EPS)*\\\n",
    "      (min(replay_start_time, ANNEALING)/float(ANNEALING))\n",
    "    return epsilon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "#   Agent Training\n",
    "# ===========================\n",
    "\n",
    "\n",
    "def train(sess, env, option_critic):  # , critic):\n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    rng = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    option_critic.update_target_network()\n",
    "    # critic.update_target_network()\n",
    "\n",
    "    # State processor\n",
    "    state_processor = StateProcessor()\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(84, 84, rng, BUFFER_SIZE, 4)\n",
    "    frame_count = 0\n",
    "    print_option_stats = False\n",
    "    # Initialize action counter to 0\n",
    "    action_counter = [{j: 0 for j in range(\n",
    "        env.action_space.n)} for i in range(OPTION_DIM)]\n",
    "    total_reward = 0\n",
    "    counter = 0\n",
    "    for i in range(MAX_EPISODES):\n",
    "        term_probs = []\n",
    "        start_frames = frame_count\n",
    "\n",
    "        while MAX_EP_STEPS > (frame_count - start_frames):\n",
    "            # if RENDER_ENV:\n",
    "            #     env.render()\n",
    "\n",
    "            current_state = env.reset()  # note I'm using only one step, original uses 4\n",
    "            current_state = state_processor.process(sess, current_state)\n",
    "            current_state = np.stack([current_state] * 4, axis=2)\n",
    "            current_option = 0\n",
    "            current_action = 0\n",
    "            new_option = np.argmax(option_critic.predict(current_state))\n",
    "            #+ (1./(1. + i)) # state has more than 3 features in pong\n",
    "            done = False\n",
    "            termination = True\n",
    "            ep_reward = 0\n",
    "            ep_ave_max_q = 0\n",
    "            termination_counter = 0\n",
    "            since_last_term = 1\n",
    "            game_over = False\n",
    "\n",
    "            start_frame_count = frame_count\n",
    "            episode_counter = 0\n",
    "\n",
    "            while not game_over:\n",
    "                frame_count += 1\n",
    "                episode_counter += 1\n",
    "                eps = get_epsilon(frame_count)\n",
    "                if termination:\n",
    "                    if print_option_stats:\n",
    "                        print(\"terminated ------- {}\".format(since_last_term))\n",
    "\n",
    "                    termination_counter += 1\n",
    "                    since_last_term = 1\n",
    "                    current_option = np.random.randint(OPTION_DIM) \\\n",
    "                        if np.random.rand() < eps else new_option\n",
    "                else:\n",
    "                    if print_option_stats:\n",
    "                        print(\"keep going\")\n",
    "\n",
    "                    since_last_term += 1\n",
    "\n",
    "                action_probs = option_critic.predict_action(\n",
    "                    [current_state], np.reshape(current_option, [1, 1]))[0]\n",
    "                current_action = np.argmax(np.random.multinomial(1, action_probs))\n",
    "                if print_option_stats:\n",
    "                    if print_option_stats:\n",
    "                        action_counter[current_option][current_action] += 1\n",
    "                        data_table = []\n",
    "                        option_count = []\n",
    "                        for ii, aa in enumerate(action_counter):\n",
    "                            s3 = sum([aa[a] for a in aa])\n",
    "                            if s3 < 1:\n",
    "                                continue\n",
    "\n",
    "                            print(ii, aa, s3)\n",
    "                            option_count.append(s3)\n",
    "                            print([str(float(aa[a]) / s3)[:5] for a in aa])\n",
    "                            data_table.append([float(aa[a]) / s3 for a in aa])\n",
    "                            print\n",
    "\n",
    "                        print\n",
    "\n",
    "                next_state, reward, done, info = env.step(current_action)\n",
    "                next_state = state_processor.process(sess, next_state)\n",
    "                next_state = np.append(\n",
    "                    current_state[:, :, 1:],\n",
    "                    np.expand_dims(next_state, 2),\n",
    "                    axis=2)\n",
    "                score, reward = get_reward(reward)\n",
    "                game_over = done or (frame_count-start_frame_count) > MAX_STEPS\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                replay_buffer.add_sample(current_state[:, :, -1], current_option, score, game_over)\n",
    "\n",
    "                term = option_critic.predict_termination([next_state], [[current_option]])\n",
    "                option_term_ps, Q_values = term[0], term[1]\n",
    "                ep_ave_max_q += np.max(Q_values)\n",
    "                new_option = np.argmax(Q_values)\n",
    "                randomize = np.random.uniform(size=np.asarray([0]).shape)\n",
    "                termination = np.greater(option_term_ps[0], randomize)\n",
    "                if frame_count < PRE_TRAIN_STEPS:\n",
    "                    termination = 1\n",
    "                else:\n",
    "\n",
    "                    # done in the original paper, actor is trained on current data\n",
    "                    # critic trained on sampled one\n",
    "                    _ = option_critic.train_actor(\n",
    "                        [current_state], [next_state],\n",
    "                        np.reshape(current_option, [1, 1]),\n",
    "                        np.reshape(current_action, [1, 1]),\n",
    "                        np.reshape(score, [1, 1]),\n",
    "                        np.reshape(done + 0, [1, 1]))\n",
    "\n",
    "                    if frame_count % (update_freq) == 0:\n",
    "                        if RENDER_ENV:\n",
    "                            env.render()\n",
    "                            show_state(env)\n",
    "\n",
    "                        # Keep adding experience to the memory until\n",
    "                        # there are at least minibatch size samples\n",
    "                        # if len(replay_buffer) > MINIBATCH_SIZE:\n",
    "                        current_state_batch, o_batch, score_batch, next_state_batch, done_batch = \\\n",
    "                            replay_buffer.random_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                        _ = option_critic.train_critic(\n",
    "                            current_state_batch, next_state_batch,\n",
    "                            np.reshape(o_batch, [MINIBATCH_SIZE, 1]),\n",
    "                            np.reshape(score_batch, [MINIBATCH_SIZE, 1]),\n",
    "                            np.reshape(done_batch + 0, [MINIBATCH_SIZE, 1]))\n",
    "\n",
    "                    if frame_count % (FREEZE_INTERVAL) == 0:\n",
    "                        # Update target networks\n",
    "                        print(\"updated params\")\n",
    "                        option_critic.update_target_network()\n",
    "\n",
    "                current_state = next_state\n",
    "                ep_reward += reward\n",
    "                term_ratio = float(termination_counter) / float(episode_counter)\n",
    "                term_probs.append(term_ratio)\n",
    "\n",
    "                if done:\n",
    "                    summary_str = sess.run(summary_ops, feed_dict={\n",
    "                        summary_vars[0]: ep_reward,\n",
    "                        summary_vars[1]: ep_ave_max_q / float(episode_counter),\n",
    "                        summary_vars[2]: 100*term_ratio,\n",
    "                        summary_vars[3]: total_reward,\n",
    "                        summary_vars[4]: total_reward / float(counter + 1),\n",
    "                        summary_vars[5]: (MAX_EP_STEPS - (frame_count - start_frames))\n",
    "                    })\n",
    "\n",
    "                    writer.add_summary(summary_str, i)\n",
    "                    writer.flush()\n",
    "\n",
    "                    break\n",
    "\n",
    "            term_ratio = float(termination_counter) / float(episode_counter)\n",
    "            print('| Reward: %.2i' % int(ep_reward), \" | Episode %d\" % (counter + 1), \\\n",
    "                ' | Qmax: %.4f' % (ep_ave_max_q / float(episode_counter)), \\\n",
    "                ' | Cummulative Reward: %.1f' % (total_reward / float(counter + 1)), \\\n",
    "                ' | %d Remaining Frames' % (MAX_EP_STEPS - (frame_count - start_frames)), \\\n",
    "                ' | Epsilon: %.4f' % eps, \" | Termination Ratio: %.2f\" % (100*term_ratio))\n",
    "            counter += 1\n",
    "\n",
    "def set_up_gym():\n",
    "\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env.seed(RANDOM_SEED)\n",
    "\n",
    "    if GYM_MONITOR_EN:\n",
    "        if not RENDER_ENV:\n",
    "            env = wrappers.Monitor(\n",
    "                env, MONITOR_DIR, video_callable=None, force=True\n",
    "            )\n",
    "        else:\n",
    "            env = wrappers.Monitor(env, MONITOR_DIR, force=True)\n",
    "\n",
    "    env.reset()\n",
    "    return env\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if not os.path.exists(MONITOR_DIR):\n",
    "        os.makedirs(MONITOR_DIR)\n",
    "\n",
    "    if not os.path.exists(SUMMARY_DIR):\n",
    "        os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    if state_dim == 210:\n",
    "        # state_dim *= env.observation_space.shape[1] # for grey scale\n",
    "        state_dim = 84 * 84 * 4\n",
    "    # action_bound = env.action_space.high\n",
    "    # Ensure action bound is symmetric\n",
    "    # assert(env.action_space.high == -env.action_space.low)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.set_random_seed(123456)\n",
    "        # sess, h_size, temp, state_dim, action_dim, option_dim, action_bound, learning_rate, tau\n",
    "        option_critic = OptionsNetwork(\n",
    "            sess, 512, 1, state_dim, action_dim, 8, ACTOR_LEARNING_RATE, TAU, GAMMA, clip_delta=1)\n",
    "\n",
    "        train(sess, env, option_critic)\n",
    "\n",
    "    # if GYM_MONITOR_EN:\n",
    "    #     env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = set_up_gym()\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3d9cc57c4a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_up_gym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/option_critic_Env_Python3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0e0f9641576a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    210\u001b[0m             sess, 512, 1, state_dim, action_dim, 8, ACTOR_LEARNING_RATE, TAU, GAMMA, clip_delta=1)\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m# if GYM_MONITOR_EN:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0e0f9641576a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, option_critic)\u001b[0m\n\u001b[1;32m    128\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mRENDER_ENV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                             \u001b[0mshow_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                         \u001b[0;31m# Keep adding experience to the memory until\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7334d45de512>\u001b[0m in \u001b[0;36mshow_state\u001b[0;34m(env, step, info)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s | Step: %d %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/option_critic_Env_Python3/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mclf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0mClear\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \"\"\"\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/option_critic_Env_Python3/lib/python3.5/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mclf\u001b[0;34m(self, keep_observers)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate over the copy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m             \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# removes ax from self._axstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/option_critic_Env_Python3/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mcla\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0;31m# refactor this out so it can be called in ax.set_title if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# pad argument used...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_title_offset_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_offset_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_title\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_right_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/option_critic_Env_Python3/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_set_title_offset_trans\u001b[0;34m(self, title_offset_points)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         self.titleOffsetTrans = mtransforms.ScaledTranslation(\n\u001b[1;32m   1172\u001b[0m                 \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_offset_points\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m72.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                 self.figure.dpi_scale_trans)\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_title\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_right_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0m_title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransAxes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitleOffsetTrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAtZJREFUeJzt1LENACEQwLDn99/5aJEyABT2BKmyZuYDOP23A4D3GAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAhDEAYQxAGAMQxgCEMQBhDEAYAxDGAIQxAGEMQBgDEMYAxAYnAwT18IEVGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = set_up_gym()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
